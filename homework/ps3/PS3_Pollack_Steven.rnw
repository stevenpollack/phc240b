\documentclass[10pt,titlepage]{article}

\usepackage{fancyhdr,palatino,mcgill}

%%%%% knitr code to make sure things stay inside listings box:
\usepackage{listings}
\usepackage{inconsolata}

\lhead{PHC240B}
\chead{Problem Set \#3}
\rhead{Steven Pollack -- 24112977}
\cfoot{\thepage}

\title{PHC240B \\ Problem Set \#3}
\author{Steven Pollack \\ 24112977}
\date{}

\renewcommand{\E}{\mathbb{E}}
\renewcommand{\Q}{\mathcal{Q}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\N}{\mathcal{N}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\expit}{expit}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}

\begin{document}
\maketitle
\pagestyle{empty}
\newpage
\pagestyle{fancy}

<<globalParameters,echo=FALSE,warning=FALSE,message=FALSE>>=
set.seed(1234)
opts_chunk$set(comment="#",tidy=F,warning=FALSE,message=FALSE,highlight=FALSE,echo=F,cache=T,dev='pdf',out.width="0.75\\textwidth",fig.align='center')
library(timereg)
data(melanoma)
attach(melanoma)
library(survival)
source("ggsurv.R")
library(xtable)
library(reshape2)
@

<<stayInside,echo=FALSE>>=
  options(width=60)
  listing <- function(x, options) {
     paste("\\begin{lstlisting}[basicstyle=\\ttfamily,breaklines=true]\n", x,"\\end{lstlisting}\n", sep = "")
  }
  
  knit_hooks$set(source=listing, output=listing)
@

\paragraph{\#1}
% Malignant lesions are often excised in patients with melanoma. The survival and characteristics of 205 patients having undergone this surgical procedure at Odense University Hospital in Denmark is provided in the dataset melanoma. This dataset is available as part of the timereg package. We are interested in studying the distribution of the time from surgery until death. Consider patients alive at the end of follow-up or dead from other causes to be right-censored.
\begin{enumerate}
  \item[a)]  % (a) Plot the Kaplan-Meier estimator of the survival function along with (pointwise) 95% confidence intervals. Provide the associated estimate and 95% confidence interval of the 5-year survival probability. Also compute the pointwise probability of surviving at least 5 years given survival until 2 years.
  <<prob1a,fig.cap="Kaplan-Meier estimator of survival function for melanoma data">>=

# convert status to a binary situation, indicating
# whether a patient died or not.
died.from.melanoma <- melanoma$status == 1

# make survival object and fit K-M curve
mel.surv.obj <- Surv(time=melanoma$days,event=died.from.melanoma,type="right")
km.curve <- survfit(formula=mel.surv.obj~1,conf.int=0.95,conf.type='plain')

# plot curve
plot1a <- ggsurv(km.curve) + labs(x="Days", y=expression(paste(hat(S),"(",x,")",sep="")),title="Kaplan-Meier curve for melanoma data")
show(plot1a)

# get estimate of S(5 years), assuming right continous
five.yrs <- 365*5
indx.of.nearest.time <- which.min(km.curve$time <= five.yrs) - 1
survival.estimate <- km.curve$surv[indx.of.nearest.time] # 76.9%
survival.estimate.ci <- c(km.curve$lower[indx.of.nearest.time],km.curve$upper[indx.of.nearest.time]) # (67.1%, 79.4%)

# if left continuous, use min(which(km.curve$time >= five.yrs))

# estimate P( S >= 5 | S >= 2):
two.yrs <- 365*2
indx.of.nearest.time2 <- which.min(km.curve$time <= two.yrs) - 1
survival.estimate2 <- km.curve$surv[indx.of.nearest.time2]
conditional.estimate <- survival.estimate / survival.estimate2 # 81.6%
@
  We estimate the probability of survival beyond 5 years to be \Sexpr{survival.estimate} with 95\% confidence interval (\Sexpr{paste(round(survival.estimate.ci,digits=4),collapse=", ")}). Using the same K-M estimator, we estimate $P(S \geq 5 \mid S \geq 2) \approx \Sexpr{survival.estimate2}$. 
  \item[b)] % (b) On a single figure, plot the Kaplan-Meier estimator of the survival function corresponding to the subgroups of patients with and without ulcerated lesions. Perform a logrank test to determine whether survivorship differs by ulceration status, report results and interpret your findings.
  <<prob1b,fig.cap="K-M estimator for survival functions for patients for whom ulcerations were present and absent.">>=
# stratify data and plot curves
km.curve2 <- survfit(formula=mel.surv.obj~ulc,conf.type='plain')
plot1b <- ggsurv(km.curve2,cens.col="black",CI=F) + scale_color_discrete(name="Ulcer status",breaks=c(0,1),labels=c("Absent","Present")) + scale_linetype_manual(guide=F,values=c(1,1)) + labs(x="Days",y=expression(paste(hat(S),"(",x,")",sep="")), title="K-M curves for patients with varying ulcer stati")
show(plot1b)

# do log-rank test
logrankTest.results <- survdiff(formula=mel.surv.obj~ulc,rho=0) # p-value < 1e-6
@
Using the \texttt{survdiff} function from the \texttt{survival} package, we can calculate a weighted log-rank test, and calculate a p-value using the fact that the test statistic is approximately distributed according to a $\chi^2$-distribution for $n$-large. The test results yield a statistic with value $t = \Sexpr{logrankTest.results$chisq}$, 1 degree of freedom, and $\text{p-value} = \Sexpr{pchisq(logrankTest.results$chisq,df=1,lower.tail=F)}$. Thus, we have good reason to believe that there is a difference between the survival functions for those who have ulcerations and those who do not.
there's a good reason to believe ulcerated people have different hazards.
  \item[c)] %# (c) Fit a Cox proportional hazards model for the survival time, including ulceration status, lesion thickness and gender as main-effect terms. Report a point estimate, standard error, 95% confidence interval and p-value for each parameter in your model. Carefully interpret each parameter estimate.
  <<prob1c,results='asis'>>=
cox.model <- coxph(formula=mel.surv.obj ~ ulc + sex + thick)

# scrap data
beta <- cox.model$coefficients
se <- sqrt(diag(cox.model$var))
z <- qnorm(0.975)
pvalues <- 2*pnorm(beta/se,lower.tail=F) # two-sided alternative

# place in matrix for output
prob1c.out <- cbind(beta,se,beta - z*se, beta + z*se,pvalues)
colnames(prob1c.out) <- c("coefficient", "standard error", "lower 95%", "upper 95%", "p-value")
rownames(prob1c.out) <- c("Ulceration status", "Sex", "Lesion thickness")

xtable(prob1c.out, digits=4, caption="Coefficient estimates with associated confidence intervals and p-values for Cox Proportional Hazards model")
@
As table 2 reports, the coefficients on ulceration status, and legion thickness are both statisticially significant (if you care to reject at the \Sexpr{round(sort(pvalues)[2],digits=4)} level). Even if statistical significance isn't convincing, both of these coefficients have confidence intervals that do not include 0, leading us to be skeptical of the null-hypothesis that they, too, should be 0. 

The consequence of our coefficient estimates is that we can expect a person's hazard to increase by a factor \Sexpr{exp(beta)[1]} in the presence of an ulceration. Likewise, males have hazard functions which are \Sexpr{exp(beta)[2]} fold greater than women, and for every 0.01mm increase in turmor thickness, a person's hazard increases by a factor of \Sexpr{exp(beta)[3]}.
\end{enumerate}

\paragraph{\#2.}
\begin{enumerate}
  \item[a)] First we consider the following inequality for two sequences, $a_n$, and $b_n$:
  \[
    \ord{\sup\ord{a_n} - \sup\ord{b_n}} \leq \sup\ord{a_n - b_n}
  \]
  This follows because
  \[
  \ord{a_n} \leq \ord{a_n - b_n} + \ord{a_n} \Implies \sup\ord{a_n} \leq \sup\ord{a_n - b_n} + \sup\ord{a_n}
  \]
  hence, 
  \[
  \sup\ord{a_n} - \sup\ord{b_n} \leq \sup\ord{a_n - b_n}
  \]
  But this implies that 
  \[
  \sup\ord{b_n} - \sup\ord{a_n} \geq - \sup\ord{a_n-b_n}
  \]
  Thus, 
  \[
  \ord{\sup\ord{a_n} - \sup\ord{b_n}} \leq \sup\ord{a_n - b_n}
  \]
  \newcommand{\W}{\mathbb{W}}
  \newcommand{\sD}{\sigma^{\diamond}}
  \newcommand{\V}{\mathbb{V}}
  Now, observe that $\W_{n}(t)$ may be written as
  \begin{align*}
    \W_{n}(t) &= \frac{\sqrt{n}(\log(\Lambda_n(t)) - \log(\Lambda_0(t)))}{\sD_{n}(t) - \sD_{0}(t) + \sD_{0}(t)} \\
    &= \left(\frac{1}{\sD_{n}(t)} - \frac{1}{\sD_{0}(t)}\right)\left(\frac{1}{\sqrt{n}}\SUM{i}{1}{n} \frac{\Phi_{P_0}(t)(O_i)}{\Lambda_{0}(t)} + \sqrt{n}s_n(t)\right) + \V_{n}(t) + \frac{\sqrt{n}s_n(t)}{\sD_{0}(t)} \\
    &\equiv X_{n}(t) Y_{n}(t) + \V_{n}(t) + Z_{n}(t)
  \end{align*}
  Hence, 
  \begin{align*}
  \ord{\sup_{t}\ord{\W_n(t)} - \sup_{t}\ord{\V_n(t)}} &\leq \sup_{t}\ord{\W_n(t) - \V_n(t)} \\
  &\leq \sup_{t}\ord{X_{n}(t)} \sup_{t}\ord{Y_{n}(t)} + \sup_{t}\ord{Z_n(t)}
  \end{align*}
  And since $\sup_{t}\ord{X_n(t)} \xrightarrow{P} 0$ (by assumption), Slutsky's theorem has that $\sup_{t}\ord{X_n(t)} \cdot \sup_{t}\ord{Y_n(t)} \xrightarrow{D} 0$. Moreover, the assumption that $\sup_{t}\ord{\sqrt{n}s_n(t)} \xrightarrow{P} 0$ implies that $\sup_{t}\ord{Z_n(t)}$ converges in probability to 0, as well. Thus, 
  \[
    \ord{\sup_{t}\ord{\W_n(t)} - \sup_{t}\ord{\V_n(t)}} \xrightarrow{P} 0
  \]
  and since convergence in probability implies convergence in law, we have that
  \[
    \lim_{n\to\infty} P(\sup_{t}\ord{\W_n(t)} \leq z) = H(z)
  \]
  \item[b)] This is just straight algebraic manipulation. If $\B_n(t)$ is a confidence band for $S_0(t)$, then for any $t \in J_0$
  \begin{align*}
    P(B_{n}^{-}(t) \leq S_0(t) \leq B_{n}^{+}(t)) &= P\left( \log\Lambda_n(t) + c_{\alpha} \frac{\sD_n(t)}{\sqrt{n}} \geq \log(-\log(S_0(t))) \geq \log\Lambda_n(t) - c_{\alpha} \frac{\sD_n(t)}{\sqrt{n}}\right) \\
    &= P\left(-c_{\alpha}\frac{\sD_n(t)}{\sqrt{n}} \leq \log\Lambda_0(t) - \log\Lambda_n(t) \leq c_{\alpha}\frac{\sD_n(t)}{\sqrt{n}} \right) \\
    &= P\left( \frac{\sqrt{n}\ord{\log\Lambda_n(t) - \log\Lambda_0(t)}}{\sD_n(t)} \leq c_{\alpha} \right) \\
    &= P\left( \ord{\W_n(t)} \leq c_{\alpha}\right)
  \end{align*}
  However, because this holds for all $t \in J_0$, we have that
  \[
  P(S_{0} \in \B_n) = P(\sup_{t}\ord{\W_n(t)} \leq c_{\alpha}) \approx H(c_\alpha) = 1-\alpha
  \]
  \item[c)] 
  \renewcommand{\U}{\mathbb{U}}
  Since we may write $\U_n(t)$ as
  \[
    \U_{n}(t) := \frac{\sqrt{n}\left(\frac{1}{n}\SUM{i}{1}{n}\frac{U_i \Phi_{P_0}(t)(O_i)}{\sigma_0(t)} - 0 \right)}{1}
  \]
  And from the from the the independence of $U$ and $O$ it follows that
  \begin{align*}
    \E[U \Phi_{P_0}(t)(O)] &= \E[U] \E[\Phi_{P_0}(t)(O)] = 0\\
    \var(U \Phi_{P_0}(t)(O)/\sigma_{0}(t)) &= \frac{\E[U^2] \sigma_{0}^{2}}{\sigma_0^2} = 1
  \end{align*}
  Hence, the central limit theorem dictates that $\U_n(t) \xrightarrow{\L} N(0,1)$.
  
  For the limiting distribution of $\W_n(t)$, set $\sD_n(t) \xrightarrow{P} \sD_0(t)$ and rewrite $\W_n(t)$ via
  \begin{align*}
    \W_n(t) &= \frac{\sqrt{n}(\log\Lambda_n(t) - \log\Lambda_0(t))}{\sD_0(t)} \cdot \frac{\sD_0(t)}{\sD_n(t)} \\
    &= \sqrt{n}\left(\frac{1}{n}\SUM{i}{1}{n}\frac{\Phi_{P_0}(t)(O_i)}{\sD_0(t)\Lambda_0(t)} + \frac{s_n(t)}{\sD_0(t)}\right) \, \frac{\sD_0(t)}{\sD_n(t)} \\
    &= \left(\sqrt{n}\left(\frac{1}{n}\SUM{i}{1}{n}\frac{\Phi_{P_0}(t)(O_i)}{\sigma_0(t)}\right) + \frac{\sqrt{n}s_n(t)}{\sD_0(t)}\right) \frac{\sD_0(t)}{\sD_n(t)} \\
    &\equiv (\sqrt{n}X_n + Y_n) Z_n
  \end{align*}
  However, from the consistency of $\sD_n(t)$, we have the $Z_n \xrightarrow{P} 1$, from the asymptotic linearity of $\log\Lambda_n(t)$ it follows that $Y_n \xrightarrow{P} 0$, and from the central limit theorem $\sqrt{n}X_n \xrightarrow{\L} N(0,1)$. Hence, two applications of slutsky's theorem show that $\W_n(t) \xrightarrow{\L} N(0,1)$. Thus, $\U_n(t)$ and $\W_n(t)$ have the same asymptotic distribution.
  \item[d)]
  <<prob2d,results='asis',cache=T>>=
  load(file="2013-10-30-15:27:17-simstudy.Rdata")
  colnames(out) <- paste("n=",n,sep="")
  print(xtable(out,caption=paste("Empirical coverage probability for different types of confidence bands; $n$ is number of sampled $T_i$'s, $m = ", m, "$ is number of sampled $U_{i}$'s.", sep=""),digits=3))
  @
  As table 2 indicates, the coverage for the band created by slapping together 95\% confidence intervals is well under 95\% (in fact, it seems to converge around \Sexpr{round(out[2,5]*100)}\%). Alternatively, and unsurprisingly, the uniform confidence band developed in parts a through c has coverage hanging slightly under 95\%.
  
  If you change the number of $U_{i}$'s to 5000, and consider $n=5000$, one finds empirical coverage probabilities around \Sexpr{round(out[1,5]*100)}\% and \Sexpr{round(out[2,5]*100)}\% for the uniform and pointwise bands (respectively). This gives us reason to believe the uniform band construction works as advertised.
  \item[e)] Say you have independent samples $O^{1}_1, \ldots, O^{1}_n$ and $O^{2}_{1}, \ldots, O^{2}_{m}$, both right-censored data, and you suppose they both have identical survival functions. Then, you could pool the samples and under the null hypothesis, the monte carlo approach should yield a band that has approximately 95\% coverage. If your simulation yields coverage that is far from 95\%, you have reason to believe your samples did not come from the same distribution.
\end{enumerate}

\paragraph{\#3.}
\begin{enumerate}
  \item[a)] Considering that
  \begin{align*}
    m_{j}(k) &= \SUM{i}{1}{n} \delta_i I(y_i = y_j, z_i = k) \\
             &= \delta_{j} I(z_j = k) \\
             &= k\delta_j z_j + (1-k) \delta_j (1-z_j)
  \end{align*}
  We see that for $\delta_j = 1$, $m_{j}(1) = z_j$. Moreover,
  \[
  m_{j}(0) + m_{j}(1) = \delta_j = 1
  \]
  Using these facts, and the identity
  \[
    \SUM{i}{1}{n} I(y_i \geq y_j) = \SUM{i}{1}{n} I(y_i \geq y_j, z_i = 1) + I(y_i \geq y_j, z_i = 0) = R_{j}(0) + R_{j}(1) := R_{j}
  \]
  we have that (for $g_0(x) = e^{x}$)
  \begin{align*}
    && PL(\beta) &= \exp\set{\SUM{i}{1}{n} \delta_i \log\left(\dfrac{g_0(\beta z_i)}{\SUM{j}{1}{n}I(y_j\geq y_i)g_{0}(\beta z_j)}\right)} \\
    &\Implies& \log(PL) &= \SUM{i}{1}{n}\delta_i \beta z_i - \delta_i\log\left(\SUM{j}{1}{n} I(y_j \geq y_i) e^{\beta z_j}\right) \\
    &\Implies& \dfrac{\partial}{\partial\beta} \log(PL(\beta))\biggr|_{\beta=0} &= \SUM{i}{1}{n}\delta_i\left(z_i - \dfrac{R_{i}(1)}{R_i}\right) \\
    && &= \SUM{i}{1}{n}\delta_i\left(m_{j}(1) - \dfrac{R_i - R_{i}(0)}{R_i}\right) \\
    && &= \SUM{i}{1}{n}\delta_i\left(1 - m_{j}(0) - 1 + \dfrac{R_{i}(0)}{R_i} \right) \\
    && &= -\SUM{i}{1}{n}\delta_i\left(m_{j}(0) - \dfrac{R_{i}(0)}{R_i} \right) \\
  \end{align*}
  Hence, 
  \[
    U_{n}(0) = \frac{1}{n}\dfrac{\partial}{\partial\beta} \log(PL(\beta))\biggr|_{\beta=0} = -\frac{1}{n}\SUM{i}{1}{n}\delta_i\left(m_{j}(0) - \dfrac{R_{i}(0)}{R_i} \right)
  \]
  \item[b)] From part a), we have that
  \[
    U_{1}(\beta) = \frac{\partial}{\partial\beta}\ell(\beta) = \SUM{i}{1}{n}\delta_i \left( z_i - \dfrac{\SUM{j}{1}{n} I(y_j\geq y_i, z_j=1) e^{\beta z_j}}{\SUM{j}{1}{n}I(y_j\geq y_i)e^{\beta z_j}}\right)
  \]
  Hence,
  \begin{align*}
    U_{1}'(0) &= \SUM{i}{1}{n}\delta_i \left( \dfrac{\SUM{j}{1}{n} I(y_j\geq y_i, z_j=1) z_j e^{\beta z_j}}{\SUM{j}{1}{n}I(y_j\geq y_i)e^{\beta z_j}} \right. \\
    & \qquad \left. - \dfrac{\left(\SUM{j}{1}{n} I(y_j\geq y_i, z_j=1) e^{\beta z_j}\right)\left(\SUM{j}{1}{n}I(y_j\geq y_i) z_j e^{\beta z_j}\right)}{\left(\SUM{j}{1}{n}I(y_j\geq y_i)e^{\beta z_j}\right)^{2}} \right) \biggr|_{\beta=0} \\
    &= \SUM{i}{1}{n}\delta_i \left( \dfrac{\left(\SUM{j}{1}{n} I(y_j\geq y_i, z_j=1) z_j \right) \left(\SUM{j}{1}{n}I(y_j\geq y_i)\right)}{\left(\SUM{j}{1}{n}I(y_j\geq y_i)\right)^{2}} \right. \\
    & \qquad \left. - \frac{\left(\SUM{j}{1}{n} I(y_j\geq y_i, z_j=1) \right)\left(\SUM{j}{1}{n}I(y_j\geq y_i, z_j=1) \right)}{\left(\SUM{j}{1}{n}I(y_j\geq y_i)\right)^{2}} \right)  \\
    &= \SUM{i}{1}{n}\delta_i \left( \dfrac{R_{i}(1) R_{i} -  R_{i}(1)^2}{R_{i}^{2}} \right) \\
    &= \SUM{i}{1}{n}\delta_i \left( \dfrac{R_{i}(1)R_{i}(0)}{R_{i}^{2}} \right) \\
  \end{align*}
  Where we used the fact that $I(y_j \geq y_i, z_j = 1) z_j = I(y_j\geq y_i, z_j=1)$ to perform the last few manipulations.
  Thus, 
  \[
   \tau_n^2 := \frac{\partial}{\partial\beta}U_{n}(0) = \frac{1}{n}\SUM{i}{1}{n}\delta_i \left( \dfrac{R_{i}(1)R_{i}(0)}{R_{i}^{2}} \right) 
  \]
  \item[c)] Generally, when testing for equal survivorship between two covariate groups, $A$, and $B$, the weighted logrank test would take on the form
  \[
    t_n = \SUM{j}{1}{d} w_n(v_j) \left[ m_{j}(A) - \frac{R_{j}(A)}{R_{j}} m_j \right]
  \]
  where $v_j$ is the $j^{th}$ observed failure time, $m_j(A)$ counts the number of uncensored deaths at time $j$ in covariate group $A$ and $R_j(A)$ is the count of the number of people who have yet to die (or drop out) as of time point $v_j$ within covariate group $A$. The standard logrank test takes $w_n(v_j) = 1$. 
  
  For our data, covariate status is binary (e.g. $z_i \in \set{0,1}$), and there are no ties between the $y_i$'s. Hence, we can substitute $\delta_j$ for $w_n(v_j)$ above, and sum over all observations, instead of separating those that were uncensored:
  \[
  t_n = \SUM{j}{1}{n} \delta_j \left[ m_{j}(0) - \frac{R_{j}(0)}{R_{j}} m_j \right]
  \]
  However, since $m_j = 1$, in our case, we see that $t_n = - U_{1}(0)$. 
  
  Thus, it follows from the fact that our score test will be most powerful when the true parameter, $\beta_0$, is near 0 that the standard logrank test will also be most powerful in a small neighborhood around 0.\footnote{full disclosure: I used wikipedia tell me about the score test\ldots}
  \item[d)] Figure 3 demonstrates that the Cox model-based estimate of survival for patients with ulcerations isn't too far off from the K-M estimator. % I'm not sure what we're supposed to take away from this. The cox model-based curves don't criss cross, which means the can expect the logrank test to be greater than zero (since we won't have cancellation in the summation). However, I'm not sure if I see a visual connection between the figure and the score test...
  
  <<prob3d,fig.cap="Cox model estimates of survival curves for patients with (and without) ulcerations, superimposed over K-M estimates.">>=
  cox.model2 <- coxph(formula=mel.surv.obj~ulc)
  cox.model2.fitted <- survfit(cox.model2,newdata=data.frame(ulc=c(0,1)))
  fitted.df <- data.frame(time=c(0,cox.model2.fitted$time),absent=c(1,cox.model2.fitted$surv[,1]),present=c(1,cox.model2.fitted$surv[,2]))
  
  fitted.melted <- melt(fitted.df,id.vars=c("time"))
  
  plot3d <- plot1b + geom_step(data=fitted.melted, aes(group=NULL,x=time,y=value, color=variable)) + scale_color_discrete(name="",breaks=c("0","1","absent","present"),labels=c("absent\nKM estimate","present\nKM estimate","absent\nCox model","present\nCox model")) + labs(title="Estimated survival curves for presence of ulceration") + theme(legend.position="top")
  
  show(plot3d)
  @
\end{enumerate}

\paragraph{\#4.}
\begin{enumerate}
  \item[a)] Consider the following:
  \begin{align*}
    \E[\Delta \mid Z = z] &= \E[ I(T \leq C) \mid Z = z] \\
    &= P( T \leq C \mid Z = z) \\
    &= \int_{t=0}^{\infty} \int_{c=t}^{\infty} f_{T,C|Z}(t,c\mid z) \, dc \, dt \\
    &= \int_{0}^{\infty} f_{T|Z}(t | z) P(C \geq t \mid Z=z) \, dt \tag{a}\\
    &= \int_{0}^{\infty} \lambda(t | z) S(t | z ) Q(t | z) \, dt
\intertext{Thus, if we perform integration by parts with $u = S(t|z) Q(t|z)$ and $dv = \lambda(t|z) dt$, we have}
    \E[\Delta \mid Z= z] &= \Lambda(x | z) S(x | z) Q(x | z) \biggr|_{x=0}^{\infty} + \int_{0}^{\infty} \Lambda(x | z)\left( q(x | z)S(x | z) + f(x | z) Q(x | z) \right) dx \\
    &= \int_{0}^{\infty} \Lambda(x | z) \left( f_{Y,\Delta|Z}(x,0|z) + f_{Y,\Delta|Z}(x,1|z)\right) \, dx \tag{b} \\
    &= \int_{0}^{\infty} \Lambda(x | z) \left(\int_{i=0}^{1} f_{Y,\Delta|Z}(x,i|z)  \, d\delta \right) \, dx \tag{c} \\
    &= \int_{0}^{\infty} \Lambda(x | z) f_{Y|Z}(x|z) \, dx \\
    &= \E[ \Lambda(Y) \mid Z = z] \\
    &= \psi(z) \E[\Lambda_{0}(Y) \mid Z= z] \tag{d}
  \end{align*}
  Where (a) follows from the conditional independence of $T$ and $C$, given $Z$, (b) follows from the identity that $S(t) = \exp(-\Lambda(t))$, (c) uses $d\delta$ to denote the counting measure, and (d) comes from the model assumption that $\lambda(t|z) = \lambda_{0}(t) \psi(z)$. Hence, 
  \[
    \psi(z) = \frac{\E[\Delta \mid Z=z]}{\E[\Lambda_{0}(Y) \mid Z= z]}
  \]
  \item[b)] 
  %%% do delta method
  Notice that
  \begin{align*}
  \psi(z) &= \frac{\E[\Delta \mid Z= z]}{\E[\Lambda_{0}(Y) \mid Z = z]} \\
          &= \dfrac{ 
                \left(\dfrac{P(\Delta = 1, Z=z)}{P(Z=z)}\right)
                }{
                \left(\dfrac{\E[\Lambda_{0}(Y)I(Z=z)]}{P(Z=z)}\right)
                } \\
          &= \frac{
                    P(\Delta=1, Z=z)
                  }{
                    \E[\Lambda_{0}(Y)I(Z=z)]
                  } := \frac{p_z}{\mu_z}
  \end{align*}
  Hence, if we take 
  \begin{align*}
  \hat{p}_{z} &= \frac{1}{n}\SUM{i}{1}{n} \delta_i I(z_i=z) \\
  \hat{\mu}_{z} &= \frac{1}{n} \SUM{i}{1}{n} \Lambda_{0}(y_i) I(z_i=z)
  \end{align*}
  the law of large numbers gives us two estimators which are consistent for $p_z$ and $\mu_z$ (respectively) and therfore $ \hat{\psi}(z) := \hat{p}_z / \hat{\mu}_z \xrightarrow{P} p_z / \mu_z$. 
  
  The influence curve for $\hat{p}_{z'}$ is $IC_1(o) = \delta I(z=z') - p_{z'}$, and the influence curve for $\hat{\mu}_{z'}$ is $IC_{2}(o) = \Lambda_0(y)I(z=z') - \mu_{z'}$. Thus, the covariance matrix for the estimators is 
  \begin{align*}
  \Sigma &\equiv \begin{pmatrix}
  \E[IC_{1}^2(O)] & \E[IC_{1}(O)IC_{2}(O)] \\
  \E[IC_{1}(O)IC_{2}(O)] & \E[IC_{2}^{2}(O)]
  \end{pmatrix} \\
  &= \begin{pmatrix}
  p_{z'}(1-p_{z'}) & p_{z'}(\E(\Lambda_{0}(Y) | \Delta=1, Z=z')-\mu_{z'}) \\
  p_{z'}(\E(\Lambda_{0}(Y) | \Delta=1, Z=z')-\mu_{z'}) & P(Z=z')\E(\Lambda_0^2(Y)|Z=z') - \mu_{z'}^2
  \end{pmatrix} \\
  &\equiv \begin{pmatrix}
  p_{z'}(1-p_{z'}) & p_{z'}(a-\mu_{z'}) \\
  p_{z'}(a-\mu_{z'}) & b - \mu_{z'}^2
  \end{pmatrix}
  \end{align*}
  Hence, if $g(x,y) = x/y$, and $\nabla g = (y^{-1}, -xy^{-2})$, then the multivariate delta method tells us that $\widehat{\psi}(z') = g(\hat{p}_{z'}, \hat{\mu}_{z'})$ is asymptotically linear with asymptotic variance equal to 
  \begin{align*}
  \nabla g(p_{z'},\mu_{z'})^{T} \Sigma \nabla g(p_{z'},\mu_{z'}) &= 
  \begin{pmatrix} \dfrac{1}{\mu_{z'}} & -\dfrac{\psi(z')}{\mu_{z'}} \end{pmatrix}
  \begin{pmatrix}
  p_{z'}(1-p_{z'}) & p_{z'}(a-\mu_{z'}) \\
  p_{z'}(a-\mu_{z'}) & b - \mu_{z'}^2
  \end{pmatrix}
  \begin{pmatrix} \dfrac{1}{\mu_{z'}} \\ -\dfrac{\psi(z')}{\mu_{z'}} \end{pmatrix} \\
  %&= \psi(z')\left(\frac{1}{\mu_{z'}} - \psi(z')\right) - 2\psi^2(z')\left(\frac{a}{\mu_{z'}} - \psi(z')\right)  - \psi^2(z')\left( \frac{\E[\Lambda^2_0(Y) | Z=z']}{\E[\Lambda_0(Y)|Z=z']} - \frac{1}{\mu_{z'}}\right) \\
  &= \frac{\psi(z')}{\mu_{z'}} + \frac{b \psi^{2}(z')}{\mu_{z'}^2} - \frac{2a\psi^2(z')}{\mu_{z'}} \\
  &= \frac{\psi(z')}{\mu_{z'}}\left( 1 + \psi(z')(c-2a)\right) \\
  &\equiv \Psi(z')
  \end{align*}
  where $c = \E[\Lambda_{0}^{2}(Y) | Z=z'] / \E[\Lambda_{0}(Y) | Z=z']$.
  
  Thus, $\sqrt{n}(\widehat{\psi}(z')-\psi(z')) \xrightarrow{D} N(0, \Psi(z'))$, by the central limit theorem.
  \item[c)] Since we're assuming $\psi(0) = 1$, we may estimate $\Lambda_0$ by subsetting our available data around observations inside the covariate group $z=0$, and using the Nelson-Aalen estimator on this dataset. That is, let
  \[
    \hat{\Lambda}_0(t) = \SUM{i}{1}{n} I(z_i = 0) \delta_i \left( \frac{I(0 \leq y_i \leq t)}{\SUM{j}{1}{n} I(y_j \geq y_i)}\right)
  \]
  From the properties of the Nelson-Aalen estimator, we may plug this estimator into $\hat{\psi}(z)$ above and obtain another estimator which is, also, consistent for $\psi(z)$. That is, 
  \[
    \widehat{\psi}^{*}(z) = \frac{\SUM{i}{1}{n} \delta_i I(z_i=z)}{\SUM{i}{1}{n} \hat{\Lambda}_{0}(y_i) I(z_i=z)}
  \]
  is consistent for $\psi(z)$. 
  \item[d)] 
  Notice that the size of $r$ may have a dramatic effect on the amount of observations with $z= 0$, and the strength of the estimate in (c) relies on a healthy number of observations with $z=0$ to estimate the baseline cumulative hazard. Outside of this issue, and for both estimators in (b) and (b), in both the small and large-sample situation, we are relying on the law of large numbers to give us reasonable estimates of $\psi(z)$. This property, like the Nelson-Aalen estimator, can't be expected to work well when there are positivity-violations, or near-positivity issues. So, in short: $r$ is important, but perhaps not as important as the number of observations in each category, as they, individually dictate the (point-wise) rate of convergence of $\hat{\psi}(z)$ and $\widehat{\psi}^{*}(z)$. 
  
  These estimates cannot be expected to work when $z$ is continuous, as we'll surely have positively violations. In the case of a continuous $z$, some coarsening would have to be applied.
  \item[e)] %If you have access to software for fitting a Cox model with standard link g 0 (u) = exp(u), describe how to obtain a point estimate and confidence interval of each component of (ψ(0), ψ(1), . . . , ψ(r)) using software output alone?
  If we model the conditional hazard via
    \[
      \lambda(t|z) = \lambda_{0}(t)\exp\left[\SUM{i}{0}{r}\beta_i I(z=i)\right]
    \]
  Then provided the software can output $\hat{\beta}_i$ and an associated confidence interval $(\hat{\ell}_i, \hat{u}_i)$, we estimate $\widehat{\psi}(i) = \exp(\hat{\beta}_i)$ with estimated confidence interval $(\exp(\hat{\ell}_i), \exp(\hat{u}_i))$. 
  \item[f)] %Suppose that the four subpopulations of patients defined by ulceration status and gender have proportional hazard functions. Taking as baseline comparison group the subpopulation of female patients without ulcerated lesions, provide point estimates of the hazard ratio for each of the other three subpopulations using i) the estimator in (c) and ii) standard software for fitting a Cox model.
  See table 2.
  <<prob4f,results='asis'>>=
  baseline.pop <- subset(x=melanoma,subset={sex==0 & ulc==0})
  baseline.pop <- transform(baseline.pop,died={status == 1})
  baseline.pop.fit <- survfit(Surv(time=days,event=died,type="right")~1,data=baseline.pop)
  n.a.est <- cumsum(baseline.pop.fit$n.event/baseline.pop.fit$n.risk)
  nelson.aalen.est <- stepfun(x=baseline.pop.fit$time,y=c(0,n.a.est))

  cov.levels <- as.matrix(expand.grid(c(0L,1L),c(0L,1L)))
  colnames(cov.levels) <- c("ulc","sex")
  psi.hat.star <- Vectorize(function(z,data=melanoma) {
    ### z = 0,1,2,...,r
    cov.group <- cov.levels[z+1,]
    subpop <-subset(x=data,subset={ulc==cov.group[1] & sex==cov.group[2]})
    num <- sum(subpop$status == 1)
    denom <- sum(nelson.aalen.est(subpop$days))
    return(num/denom)
  })
  ### transform data to make 3 dummy variables
  dummy.data <- within(data=melanoma,expr={
    Z1 <- (ulc==1 & sex==0)
    Z2 <- (ulc==0 & sex==1)
    Z3 <- (ulc==1 & sex==1)
    uncensored <- (status == 1)
    })
  dummy.cox <- coxph(formula=Surv(days,event=uncensored)~Z1+Z2+Z3,data=dummy.data)
  prob4f.out <- cbind(cov.levels,psi.hat.star(0:3),c(1,exp(dummy.cox$coefficients)))
  colnames(prob4f.out) <- c("ulc","sex","non-parametric estimates", "cox model estimates")
  prob4f.out[-1,1:2] <- cbind(c("present","absent","present"), c("female","male","male"))
  prob4f.out[,3:4] <- round(as.numeric(prob4f.out[,3:4]),digits=4)
  print(xtable(prob4f.out[-1,], caption="Coefficient estimates from estimator in part c, as well as \\texttt{coxph}"),include.rownames=F)
  @
\end{enumerate}
\end{document}